{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25eafcfd",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - 03 Preprocessing and Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9db829",
   "metadata": {},
   "source": [
    "The objective of this notebook is to prepare data for fitting models. In notebook 02_Exploratory_Data_Analysis, we have already featured new columns for the stock data, measuring the changes each day. In this notebook, we will be using word embedding to convert words to vectors, then use that as an additional parameter to our training set. Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions.\n",
    "\n",
    "Guidance from Springboard:\n",
    "\n",
    "Create dummy or indicator features for categorical variables \\\n",
    "Standardize the magnitude of numeric features using a scaler \\\n",
    "Split your data into testing and training datasets\n",
    "\n",
    "**Link(s) to previous notebook(s)**: \\\n",
    "00_Historical_Data_2014: https://github.com/parisvu07/Springboard_Data_Science/tree/main/Capstone_2_Twitter_Sentiment_Analysis \\\n",
    "01_Data_Wrangling:\n",
    "https://github.com/parisvu07/Springboard_Data_Science/blob/main/Capstone_2_Twitter_Sentiment_Analysis/01_Data_Wrangling.ipynb \\\n",
    "02_Exploratory_Data_Analysis: https://github.com/parisvu07/Springboard_Data_Science/blob/main/Capstone_2_Twitter_Sentiment_Analysis/02_Exploratory_Data_Analysis.ipynb\n",
    "\n",
    "Quick fix for \"Unable to render rich display\": copy and paste the notebook link to https://nbviewer.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf7e24",
   "metadata": {},
   "source": [
    "## 3.1 Combining Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff48907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "LabeledSentence = gensim.models.doc2vec.TaggedDocument\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from nltk.tokenize import TweetTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6210f2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>user</th>\n",
       "      <th>likes</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:29:43</td>\n",
       "      <td>LlcBillionaire</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>10 New Year’s food traditions around the world</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:29:32</td>\n",
       "      <td>skitontop1</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>Entries &amp;amp; exits Daily! \\nDiscord link belo...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:29:28</td>\n",
       "      <td>StockJobberOG</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>$AAPL $MSFT $SPY $TSLA $AMZN $BRK.B\\n\\n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:29:11</td>\n",
       "      <td>LlcBillionaire</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>The biggest — and maybe the best — financial r...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:28:29</td>\n",
       "      <td>skitontop1</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>#1 Chatroom interms of \\n\\nalert,calls,Analysi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dates      Time            user  likes           source  \\\n",
       "0  2022-12-30  20:29:43  LlcBillionaire      0  Twitter Web App   \n",
       "1  2022-12-30  20:29:32      skitontop1      0  Twitter Web App   \n",
       "2  2022-12-30  20:29:28   StockJobberOG      0  Twitter Web App   \n",
       "3  2022-12-30  20:29:11  LlcBillionaire      0  Twitter Web App   \n",
       "4  2022-12-30  20:28:29      skitontop1      0  Twitter Web App   \n",
       "\n",
       "                                                text  Subjectivity  Polarity  \\\n",
       "0    10 New Year’s food traditions around the world       0.454545  0.136364   \n",
       "1  Entries &amp; exits Daily! \\nDiscord link belo...      0.500000  0.300000   \n",
       "2            $AAPL $MSFT $SPY $TSLA $AMZN $BRK.B\\n\\n      0.000000  0.000000   \n",
       "3  The biggest — and maybe the best — financial r...      0.150000  0.500000   \n",
       "4  #1 Chatroom interms of \\n\\nalert,calls,Analysi...      1.000000  0.600000   \n",
       "\n",
       "   Analysis  Sentiment  \n",
       "0  Positive        1.0  \n",
       "1  Positive        1.0  \n",
       "2   Neutral        0.0  \n",
       "3  Positive        1.0  \n",
       "4  Positive        1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing tweet data from previous notebook \"02_Exploratory_Data_Analysis\"\n",
    "trading_hours_tweets = pd.read_csv('02_tweets_data.csv', lineterminator='\\n')\n",
    "trading_hours_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb37674a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>%_change_Open</th>\n",
       "      <th>%_change_High</th>\n",
       "      <th>%_change_Low</th>\n",
       "      <th>%_change_Close</th>\n",
       "      <th>%_change_Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>148.309998</td>\n",
       "      <td>71250400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02</th>\n",
       "      <td>147.809998</td>\n",
       "      <td>65447400</td>\n",
       "      <td>-1.518116</td>\n",
       "      <td>-0.757731</td>\n",
       "      <td>-0.654803</td>\n",
       "      <td>-0.337132</td>\n",
       "      <td>-8.144516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-05</th>\n",
       "      <td>146.630005</td>\n",
       "      <td>68826400</td>\n",
       "      <td>1.240064</td>\n",
       "      <td>1.972972</td>\n",
       "      <td>0.082396</td>\n",
       "      <td>-0.798317</td>\n",
       "      <td>5.162925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-06</th>\n",
       "      <td>142.910004</td>\n",
       "      <td>64727200</td>\n",
       "      <td>-0.473707</td>\n",
       "      <td>-2.398619</td>\n",
       "      <td>-2.641151</td>\n",
       "      <td>-2.536999</td>\n",
       "      <td>-5.955854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-07</th>\n",
       "      <td>140.940002</td>\n",
       "      <td>69721100</td>\n",
       "      <td>-3.318151</td>\n",
       "      <td>-2.668030</td>\n",
       "      <td>-1.352874</td>\n",
       "      <td>-1.378491</td>\n",
       "      <td>7.715304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Adj Close    Volume  %_change_Open  %_change_High  %_change_Low  \\\n",
       "Dates                                                                          \n",
       "2022-12-01  148.309998  71250400            NaN            NaN           NaN   \n",
       "2022-12-02  147.809998  65447400      -1.518116      -0.757731     -0.654803   \n",
       "2022-12-05  146.630005  68826400       1.240064       1.972972      0.082396   \n",
       "2022-12-06  142.910004  64727200      -0.473707      -2.398619     -2.641151   \n",
       "2022-12-07  140.940002  69721100      -3.318151      -2.668030     -1.352874   \n",
       "\n",
       "            %_change_Close  %_change_Volume  \n",
       "Dates                                        \n",
       "2022-12-01             NaN              NaN  \n",
       "2022-12-02       -0.337132        -8.144516  \n",
       "2022-12-05       -0.798317         5.162925  \n",
       "2022-12-06       -2.536999        -5.955854  \n",
       "2022-12-07       -1.378491         7.715304  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing stock data from notebook \"02_Exploratory_Data_Analysis\"\n",
    "eda_stock_data = pd.read_csv('02_stock_data.csv', encoding='latin-1')\n",
    "eda_stock_data = eda_stock_data.set_index('Dates')\n",
    "eda_stock_data = eda_stock_data.drop('Time', axis=1)\n",
    "eda_stock_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cebbf5",
   "metadata": {},
   "source": [
    "## 3.1 Pre-processing: Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44587ea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#renaming Volume to stock_volume\n",
    "eda_stock_data = eda_stock_data.rename(columns={'Volume': 'stock_volume'})\n",
    "\n",
    "#find twitter volume during trading hours\n",
    "eda_stock_data['twitter_volume'] = trading_hours_tweets.groupby('Dates')['text'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d6998",
   "metadata": {},
   "source": [
    "### 3.1.2 Binary Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f6f6f",
   "metadata": {},
   "source": [
    "Creating a new columns to measure stock trend. If today change in price is greater than yesterday price, trend equals to 1. If it is less than, stock trend equals 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93b7b49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>stock_volume</th>\n",
       "      <th>twitter_volume</th>\n",
       "      <th>open_trend</th>\n",
       "      <th>high_trend</th>\n",
       "      <th>low_trend</th>\n",
       "      <th>close_trend</th>\n",
       "      <th>volume_trend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>148.309998</td>\n",
       "      <td>71250400</td>\n",
       "      <td>1451</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02</th>\n",
       "      <td>147.809998</td>\n",
       "      <td>65447400</td>\n",
       "      <td>1551</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-05</th>\n",
       "      <td>146.630005</td>\n",
       "      <td>68826400</td>\n",
       "      <td>1738</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-06</th>\n",
       "      <td>142.910004</td>\n",
       "      <td>64727200</td>\n",
       "      <td>2072</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-07</th>\n",
       "      <td>140.940002</td>\n",
       "      <td>69721100</td>\n",
       "      <td>1912</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Adj Close  stock_volume  twitter_volume  open_trend  high_trend  \\\n",
       "Dates                                                                          \n",
       "2022-12-01  148.309998      71250400            1451           0           0   \n",
       "2022-12-02  147.809998      65447400            1551           0           0   \n",
       "2022-12-05  146.630005      68826400            1738           1           1   \n",
       "2022-12-06  142.910004      64727200            2072           0           0   \n",
       "2022-12-07  140.940002      69721100            1912           0           0   \n",
       "\n",
       "            low_trend  close_trend  volume_trend  \n",
       "Dates                                             \n",
       "2022-12-01          0            0             0  \n",
       "2022-12-02          0            0             0  \n",
       "2022-12-05          1            0             1  \n",
       "2022-12-06          0            0             0  \n",
       "2022-12-07          0            0             1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a new columns representing trends\n",
    "RISE = 1\n",
    "FALL = 0\n",
    "\n",
    "#Make a copy of the eda_stock_data dataset\n",
    "stock_data = eda_stock_data.copy()\n",
    "stock_data['open_trend'] = np.where(stock_data['%_change_Open'] > 0, RISE, FALL)\n",
    "stock_data['high_trend'] = np.where(stock_data['%_change_High'] > 0, RISE, FALL)\n",
    "stock_data['low_trend'] = np.where(stock_data['%_change_Low'] > 0, RISE, FALL)\n",
    "stock_data['close_trend'] = np.where(stock_data['%_change_Close'] > 0, RISE, FALL)\n",
    "stock_data['volume_trend'] = np.where(stock_data['%_change_Volume'] > 0, RISE, FALL)\n",
    "\n",
    "#Drop unnecessary columns\n",
    "stock_data = stock_data.drop(['%_change_Open','%_change_High','%_change_Low','%_change_Close','%_change_Volume'], axis=1)\n",
    "\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c052d1",
   "metadata": {},
   "source": [
    "Next, we will combine the twitter dataframe and stock dataframe to find the overall sentiment at the end of each trading day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9dafb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>3.358270</td>\n",
       "      <td>0.341031</td>\n",
       "      <td>0.166630</td>\n",
       "      <td>0.418668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02</th>\n",
       "      <td>2.422508</td>\n",
       "      <td>0.336724</td>\n",
       "      <td>0.179263</td>\n",
       "      <td>0.434727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-03</th>\n",
       "      <td>4.091234</td>\n",
       "      <td>0.338026</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.075134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-04</th>\n",
       "      <td>5.201709</td>\n",
       "      <td>0.351474</td>\n",
       "      <td>0.212572</td>\n",
       "      <td>0.491453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-05</th>\n",
       "      <td>16.589788</td>\n",
       "      <td>0.285005</td>\n",
       "      <td>0.119601</td>\n",
       "      <td>0.320138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-06</th>\n",
       "      <td>3.363636</td>\n",
       "      <td>0.308533</td>\n",
       "      <td>0.138852</td>\n",
       "      <td>0.345839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-07</th>\n",
       "      <td>3.910183</td>\n",
       "      <td>0.306545</td>\n",
       "      <td>0.141816</td>\n",
       "      <td>0.385379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-08</th>\n",
       "      <td>2.735952</td>\n",
       "      <td>0.323832</td>\n",
       "      <td>0.177100</td>\n",
       "      <td>0.450755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-09</th>\n",
       "      <td>2.080395</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.172363</td>\n",
       "      <td>0.416784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-10</th>\n",
       "      <td>2.078261</td>\n",
       "      <td>0.333348</td>\n",
       "      <td>0.164011</td>\n",
       "      <td>0.447826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-11</th>\n",
       "      <td>10.576871</td>\n",
       "      <td>0.360938</td>\n",
       "      <td>0.221710</td>\n",
       "      <td>0.531973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-12</th>\n",
       "      <td>2.265884</td>\n",
       "      <td>0.307966</td>\n",
       "      <td>0.150830</td>\n",
       "      <td>0.387431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-13</th>\n",
       "      <td>2.004828</td>\n",
       "      <td>0.317893</td>\n",
       "      <td>0.169065</td>\n",
       "      <td>0.425966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-14</th>\n",
       "      <td>1.507839</td>\n",
       "      <td>0.293880</td>\n",
       "      <td>0.155435</td>\n",
       "      <td>0.414451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-15</th>\n",
       "      <td>2.876294</td>\n",
       "      <td>0.312479</td>\n",
       "      <td>0.145653</td>\n",
       "      <td>0.379990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-16</th>\n",
       "      <td>3.901804</td>\n",
       "      <td>0.293488</td>\n",
       "      <td>0.147329</td>\n",
       "      <td>0.367735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-17</th>\n",
       "      <td>4.620968</td>\n",
       "      <td>0.299643</td>\n",
       "      <td>0.129450</td>\n",
       "      <td>0.379032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-18</th>\n",
       "      <td>2.649560</td>\n",
       "      <td>0.346529</td>\n",
       "      <td>0.196756</td>\n",
       "      <td>0.464809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-19</th>\n",
       "      <td>4.602342</td>\n",
       "      <td>0.294270</td>\n",
       "      <td>0.126957</td>\n",
       "      <td>0.335540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-20</th>\n",
       "      <td>3.728721</td>\n",
       "      <td>0.315957</td>\n",
       "      <td>0.149016</td>\n",
       "      <td>0.357392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-21</th>\n",
       "      <td>2.823909</td>\n",
       "      <td>0.314458</td>\n",
       "      <td>0.161636</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-22</th>\n",
       "      <td>2.067527</td>\n",
       "      <td>0.308212</td>\n",
       "      <td>0.153190</td>\n",
       "      <td>0.373333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>5.494789</td>\n",
       "      <td>0.324043</td>\n",
       "      <td>0.176336</td>\n",
       "      <td>0.428963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-24</th>\n",
       "      <td>13.122047</td>\n",
       "      <td>0.307284</td>\n",
       "      <td>0.155640</td>\n",
       "      <td>0.417323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-25</th>\n",
       "      <td>1.408092</td>\n",
       "      <td>0.341614</td>\n",
       "      <td>0.224174</td>\n",
       "      <td>0.505202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-26</th>\n",
       "      <td>1.257517</td>\n",
       "      <td>0.323131</td>\n",
       "      <td>0.190212</td>\n",
       "      <td>0.444873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>3.625591</td>\n",
       "      <td>0.316464</td>\n",
       "      <td>0.137869</td>\n",
       "      <td>0.362050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>5.331438</td>\n",
       "      <td>0.304389</td>\n",
       "      <td>0.129806</td>\n",
       "      <td>0.328312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>8.204908</td>\n",
       "      <td>0.206590</td>\n",
       "      <td>0.064580</td>\n",
       "      <td>0.176601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>6.911228</td>\n",
       "      <td>0.199473</td>\n",
       "      <td>0.060634</td>\n",
       "      <td>0.168082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                likes  Subjectivity  Polarity  Sentiment\n",
       "Dates                                                   \n",
       "2022-12-01   3.358270      0.341031  0.166630   0.418668\n",
       "2022-12-02   2.422508      0.336724  0.179263   0.434727\n",
       "2022-12-03   4.091234      0.338026  0.092043   0.075134\n",
       "2022-12-04   5.201709      0.351474  0.212572   0.491453\n",
       "2022-12-05  16.589788      0.285005  0.119601   0.320138\n",
       "2022-12-06   3.363636      0.308533  0.138852   0.345839\n",
       "2022-12-07   3.910183      0.306545  0.141816   0.385379\n",
       "2022-12-08   2.735952      0.323832  0.177100   0.450755\n",
       "2022-12-09   2.080395      0.317647  0.172363   0.416784\n",
       "2022-12-10   2.078261      0.333348  0.164011   0.447826\n",
       "2022-12-11  10.576871      0.360938  0.221710   0.531973\n",
       "2022-12-12   2.265884      0.307966  0.150830   0.387431\n",
       "2022-12-13   2.004828      0.317893  0.169065   0.425966\n",
       "2022-12-14   1.507839      0.293880  0.155435   0.414451\n",
       "2022-12-15   2.876294      0.312479  0.145653   0.379990\n",
       "2022-12-16   3.901804      0.293488  0.147329   0.367735\n",
       "2022-12-17   4.620968      0.299643  0.129450   0.379032\n",
       "2022-12-18   2.649560      0.346529  0.196756   0.464809\n",
       "2022-12-19   4.602342      0.294270  0.126957   0.335540\n",
       "2022-12-20   3.728721      0.315957  0.149016   0.357392\n",
       "2022-12-21   2.823909      0.314458  0.161636   0.380952\n",
       "2022-12-22   2.067527      0.308212  0.153190   0.373333\n",
       "2022-12-23   5.494789      0.324043  0.176336   0.428963\n",
       "2022-12-24  13.122047      0.307284  0.155640   0.417323\n",
       "2022-12-25   1.408092      0.341614  0.224174   0.505202\n",
       "2022-12-26   1.257517      0.323131  0.190212   0.444873\n",
       "2022-12-27   3.625591      0.316464  0.137869   0.362050\n",
       "2022-12-28   5.331438      0.304389  0.129806   0.328312\n",
       "2022-12-29   8.204908      0.206590  0.064580   0.176601\n",
       "2022-12-30   6.911228      0.199473  0.060634   0.168082"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_tweets = trading_hours_tweets.groupby('Dates').mean()\n",
    "combined_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7eae49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>stock_volume</th>\n",
       "      <th>twitter_volume</th>\n",
       "      <th>open_trend</th>\n",
       "      <th>high_trend</th>\n",
       "      <th>low_trend</th>\n",
       "      <th>close_trend</th>\n",
       "      <th>volume_trend</th>\n",
       "      <th>likes</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>148.309998</td>\n",
       "      <td>71250400</td>\n",
       "      <td>1451</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.358270</td>\n",
       "      <td>0.341031</td>\n",
       "      <td>0.166630</td>\n",
       "      <td>0.418668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02</th>\n",
       "      <td>147.809998</td>\n",
       "      <td>65447400</td>\n",
       "      <td>1551</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.422508</td>\n",
       "      <td>0.336724</td>\n",
       "      <td>0.179263</td>\n",
       "      <td>0.434727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-05</th>\n",
       "      <td>146.630005</td>\n",
       "      <td>68826400</td>\n",
       "      <td>1738</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.589788</td>\n",
       "      <td>0.285005</td>\n",
       "      <td>0.119601</td>\n",
       "      <td>0.320138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-06</th>\n",
       "      <td>142.910004</td>\n",
       "      <td>64727200</td>\n",
       "      <td>2072</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>0.308533</td>\n",
       "      <td>0.138852</td>\n",
       "      <td>0.345839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-07</th>\n",
       "      <td>140.940002</td>\n",
       "      <td>69721100</td>\n",
       "      <td>1912</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.910183</td>\n",
       "      <td>0.306545</td>\n",
       "      <td>0.141816</td>\n",
       "      <td>0.385379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-08</th>\n",
       "      <td>142.649994</td>\n",
       "      <td>62128300</td>\n",
       "      <td>1649</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.735952</td>\n",
       "      <td>0.323832</td>\n",
       "      <td>0.177100</td>\n",
       "      <td>0.450755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-09</th>\n",
       "      <td>142.160004</td>\n",
       "      <td>76097000</td>\n",
       "      <td>1415</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.080395</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.172363</td>\n",
       "      <td>0.416784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-12</th>\n",
       "      <td>144.490005</td>\n",
       "      <td>70462700</td>\n",
       "      <td>1441</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.265884</td>\n",
       "      <td>0.307966</td>\n",
       "      <td>0.150830</td>\n",
       "      <td>0.387431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-13</th>\n",
       "      <td>145.470001</td>\n",
       "      <td>93886200</td>\n",
       "      <td>1860</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.004828</td>\n",
       "      <td>0.317893</td>\n",
       "      <td>0.169065</td>\n",
       "      <td>0.425966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-14</th>\n",
       "      <td>143.210007</td>\n",
       "      <td>82291200</td>\n",
       "      <td>1466</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.507839</td>\n",
       "      <td>0.293880</td>\n",
       "      <td>0.155435</td>\n",
       "      <td>0.414451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-15</th>\n",
       "      <td>136.500000</td>\n",
       "      <td>98931900</td>\n",
       "      <td>2025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.876294</td>\n",
       "      <td>0.312479</td>\n",
       "      <td>0.145653</td>\n",
       "      <td>0.379990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-16</th>\n",
       "      <td>134.509995</td>\n",
       "      <td>160156900</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.901804</td>\n",
       "      <td>0.293488</td>\n",
       "      <td>0.147329</td>\n",
       "      <td>0.367735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-19</th>\n",
       "      <td>132.369995</td>\n",
       "      <td>79592600</td>\n",
       "      <td>1962</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.602342</td>\n",
       "      <td>0.294270</td>\n",
       "      <td>0.126957</td>\n",
       "      <td>0.335540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-20</th>\n",
       "      <td>132.300003</td>\n",
       "      <td>77432800</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.728721</td>\n",
       "      <td>0.315957</td>\n",
       "      <td>0.149016</td>\n",
       "      <td>0.357392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-21</th>\n",
       "      <td>135.449997</td>\n",
       "      <td>85928000</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.823909</td>\n",
       "      <td>0.314458</td>\n",
       "      <td>0.161636</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-22</th>\n",
       "      <td>132.229996</td>\n",
       "      <td>77852100</td>\n",
       "      <td>2319</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.067527</td>\n",
       "      <td>0.308212</td>\n",
       "      <td>0.153190</td>\n",
       "      <td>0.373333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>131.860001</td>\n",
       "      <td>63814900</td>\n",
       "      <td>1822</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.494789</td>\n",
       "      <td>0.324043</td>\n",
       "      <td>0.176336</td>\n",
       "      <td>0.428963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>130.029999</td>\n",
       "      <td>69007800</td>\n",
       "      <td>2745</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.625591</td>\n",
       "      <td>0.316464</td>\n",
       "      <td>0.137869</td>\n",
       "      <td>0.362050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>126.040001</td>\n",
       "      <td>85438400</td>\n",
       "      <td>3508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.331438</td>\n",
       "      <td>0.304389</td>\n",
       "      <td>0.129806</td>\n",
       "      <td>0.328312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>129.610001</td>\n",
       "      <td>75703700</td>\n",
       "      <td>37362</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.204908</td>\n",
       "      <td>0.206590</td>\n",
       "      <td>0.064580</td>\n",
       "      <td>0.176601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>129.929993</td>\n",
       "      <td>76960600</td>\n",
       "      <td>36973</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.911228</td>\n",
       "      <td>0.199473</td>\n",
       "      <td>0.060634</td>\n",
       "      <td>0.168082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Adj Close  stock_volume  twitter_volume  open_trend  high_trend  \\\n",
       "Dates                                                                          \n",
       "2022-12-01  148.309998      71250400            1451           0           0   \n",
       "2022-12-02  147.809998      65447400            1551           0           0   \n",
       "2022-12-05  146.630005      68826400            1738           1           1   \n",
       "2022-12-06  142.910004      64727200            2072           0           0   \n",
       "2022-12-07  140.940002      69721100            1912           0           0   \n",
       "2022-12-08  142.649994      62128300            1649           1           1   \n",
       "2022-12-09  142.160004      76097000            1415           0           1   \n",
       "2022-12-12  144.490005      70462700            1441           1           0   \n",
       "2022-12-13  145.470001      93886200            1860           1           1   \n",
       "2022-12-14  143.210007      82291200            1466           0           0   \n",
       "2022-12-15  136.500000      98931900            2025           0           0   \n",
       "2022-12-16  134.509995     160156900            1994           0           0   \n",
       "2022-12-19  132.369995      79592600            1962           0           0   \n",
       "2022-12-20  132.300003      77432800            2006           0           0   \n",
       "2022-12-21  135.449997      85928000            2013           1           1   \n",
       "2022-12-22  132.229996      77852100            2319           1           0   \n",
       "2022-12-23  131.860001      63814900            1822           0           0   \n",
       "2022-12-27  130.029999      69007800            2745           1           0   \n",
       "2022-12-28  126.040001      85438400            3508           0           0   \n",
       "2022-12-29  129.610001      75703700           37362           0           0   \n",
       "2022-12-30  129.929993      76960600           36973           1           0   \n",
       "\n",
       "            low_trend  close_trend  volume_trend      likes  Subjectivity  \\\n",
       "Dates                                                                       \n",
       "2022-12-01          0            0             0   3.358270      0.341031   \n",
       "2022-12-02          0            0             0   2.422508      0.336724   \n",
       "2022-12-05          1            0             1  16.589788      0.285005   \n",
       "2022-12-06          0            0             0   3.363636      0.308533   \n",
       "2022-12-07          0            0             1   3.910183      0.306545   \n",
       "2022-12-08          1            1             0   2.735952      0.323832   \n",
       "2022-12-09          0            0             1   2.080395      0.317647   \n",
       "2022-12-12          1            1             0   2.265884      0.307966   \n",
       "2022-12-13          1            1             1   2.004828      0.317893   \n",
       "2022-12-14          0            0             0   1.507839      0.293880   \n",
       "2022-12-15          0            0             1   2.876294      0.312479   \n",
       "2022-12-16          0            0             1   3.901804      0.293488   \n",
       "2022-12-19          0            0             0   4.602342      0.294270   \n",
       "2022-12-20          0            0             0   3.728721      0.315957   \n",
       "2022-12-21          1            1             1   2.823909      0.314458   \n",
       "2022-12-22          0            0             0   2.067527      0.308212   \n",
       "2022-12-23          0            0             0   5.494789      0.324043   \n",
       "2022-12-27          0            0             1   3.625591      0.316464   \n",
       "2022-12-28          0            0             1   5.331438      0.304389   \n",
       "2022-12-29          1            1             0   8.204908      0.206590   \n",
       "2022-12-30          0            1             1   6.911228      0.199473   \n",
       "\n",
       "            Polarity  Sentiment  \n",
       "Dates                            \n",
       "2022-12-01  0.166630   0.418668  \n",
       "2022-12-02  0.179263   0.434727  \n",
       "2022-12-05  0.119601   0.320138  \n",
       "2022-12-06  0.138852   0.345839  \n",
       "2022-12-07  0.141816   0.385379  \n",
       "2022-12-08  0.177100   0.450755  \n",
       "2022-12-09  0.172363   0.416784  \n",
       "2022-12-12  0.150830   0.387431  \n",
       "2022-12-13  0.169065   0.425966  \n",
       "2022-12-14  0.155435   0.414451  \n",
       "2022-12-15  0.145653   0.379990  \n",
       "2022-12-16  0.147329   0.367735  \n",
       "2022-12-19  0.126957   0.335540  \n",
       "2022-12-20  0.149016   0.357392  \n",
       "2022-12-21  0.161636   0.380952  \n",
       "2022-12-22  0.153190   0.373333  \n",
       "2022-12-23  0.176336   0.428963  \n",
       "2022-12-27  0.137869   0.362050  \n",
       "2022-12-28  0.129806   0.328312  \n",
       "2022-12-29  0.064580   0.176601  \n",
       "2022-12-30  0.060634   0.168082  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataframes = pd.merge(stock_data, combined_tweets, left_index=True, right_index=True)\n",
    "merged_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499e2118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>stock_volume</th>\n",
       "      <th>twitter_volume</th>\n",
       "      <th>likes</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>open_trend</th>\n",
       "      <th>high_trend</th>\n",
       "      <th>low_trend</th>\n",
       "      <th>close_trend</th>\n",
       "      <th>volume_trend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>148.309998</td>\n",
       "      <td>71250400</td>\n",
       "      <td>1451</td>\n",
       "      <td>3.358270</td>\n",
       "      <td>0.341031</td>\n",
       "      <td>0.166630</td>\n",
       "      <td>0.418668</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02</th>\n",
       "      <td>147.809998</td>\n",
       "      <td>65447400</td>\n",
       "      <td>1551</td>\n",
       "      <td>2.422508</td>\n",
       "      <td>0.336724</td>\n",
       "      <td>0.179263</td>\n",
       "      <td>0.434727</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-05</th>\n",
       "      <td>146.630005</td>\n",
       "      <td>68826400</td>\n",
       "      <td>1738</td>\n",
       "      <td>16.589788</td>\n",
       "      <td>0.285005</td>\n",
       "      <td>0.119601</td>\n",
       "      <td>0.320138</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-06</th>\n",
       "      <td>142.910004</td>\n",
       "      <td>64727200</td>\n",
       "      <td>2072</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>0.308533</td>\n",
       "      <td>0.138852</td>\n",
       "      <td>0.345839</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-07</th>\n",
       "      <td>140.940002</td>\n",
       "      <td>69721100</td>\n",
       "      <td>1912</td>\n",
       "      <td>3.910183</td>\n",
       "      <td>0.306545</td>\n",
       "      <td>0.141816</td>\n",
       "      <td>0.385379</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-08</th>\n",
       "      <td>142.649994</td>\n",
       "      <td>62128300</td>\n",
       "      <td>1649</td>\n",
       "      <td>2.735952</td>\n",
       "      <td>0.323832</td>\n",
       "      <td>0.177100</td>\n",
       "      <td>0.450755</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-09</th>\n",
       "      <td>142.160004</td>\n",
       "      <td>76097000</td>\n",
       "      <td>1415</td>\n",
       "      <td>2.080395</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.172363</td>\n",
       "      <td>0.416784</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-12</th>\n",
       "      <td>144.490005</td>\n",
       "      <td>70462700</td>\n",
       "      <td>1441</td>\n",
       "      <td>2.265884</td>\n",
       "      <td>0.307966</td>\n",
       "      <td>0.150830</td>\n",
       "      <td>0.387431</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-13</th>\n",
       "      <td>145.470001</td>\n",
       "      <td>93886200</td>\n",
       "      <td>1860</td>\n",
       "      <td>2.004828</td>\n",
       "      <td>0.317893</td>\n",
       "      <td>0.169065</td>\n",
       "      <td>0.425966</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-14</th>\n",
       "      <td>143.210007</td>\n",
       "      <td>82291200</td>\n",
       "      <td>1466</td>\n",
       "      <td>1.507839</td>\n",
       "      <td>0.293880</td>\n",
       "      <td>0.155435</td>\n",
       "      <td>0.414451</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-15</th>\n",
       "      <td>136.500000</td>\n",
       "      <td>98931900</td>\n",
       "      <td>2025</td>\n",
       "      <td>2.876294</td>\n",
       "      <td>0.312479</td>\n",
       "      <td>0.145653</td>\n",
       "      <td>0.379990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-16</th>\n",
       "      <td>134.509995</td>\n",
       "      <td>160156900</td>\n",
       "      <td>1994</td>\n",
       "      <td>3.901804</td>\n",
       "      <td>0.293488</td>\n",
       "      <td>0.147329</td>\n",
       "      <td>0.367735</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-19</th>\n",
       "      <td>132.369995</td>\n",
       "      <td>79592600</td>\n",
       "      <td>1962</td>\n",
       "      <td>4.602342</td>\n",
       "      <td>0.294270</td>\n",
       "      <td>0.126957</td>\n",
       "      <td>0.335540</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-20</th>\n",
       "      <td>132.300003</td>\n",
       "      <td>77432800</td>\n",
       "      <td>2006</td>\n",
       "      <td>3.728721</td>\n",
       "      <td>0.315957</td>\n",
       "      <td>0.149016</td>\n",
       "      <td>0.357392</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-21</th>\n",
       "      <td>135.449997</td>\n",
       "      <td>85928000</td>\n",
       "      <td>2013</td>\n",
       "      <td>2.823909</td>\n",
       "      <td>0.314458</td>\n",
       "      <td>0.161636</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-22</th>\n",
       "      <td>132.229996</td>\n",
       "      <td>77852100</td>\n",
       "      <td>2319</td>\n",
       "      <td>2.067527</td>\n",
       "      <td>0.308212</td>\n",
       "      <td>0.153190</td>\n",
       "      <td>0.373333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>131.860001</td>\n",
       "      <td>63814900</td>\n",
       "      <td>1822</td>\n",
       "      <td>5.494789</td>\n",
       "      <td>0.324043</td>\n",
       "      <td>0.176336</td>\n",
       "      <td>0.428963</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>130.029999</td>\n",
       "      <td>69007800</td>\n",
       "      <td>2745</td>\n",
       "      <td>3.625591</td>\n",
       "      <td>0.316464</td>\n",
       "      <td>0.137869</td>\n",
       "      <td>0.362050</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>126.040001</td>\n",
       "      <td>85438400</td>\n",
       "      <td>3508</td>\n",
       "      <td>5.331438</td>\n",
       "      <td>0.304389</td>\n",
       "      <td>0.129806</td>\n",
       "      <td>0.328312</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>129.610001</td>\n",
       "      <td>75703700</td>\n",
       "      <td>37362</td>\n",
       "      <td>8.204908</td>\n",
       "      <td>0.206590</td>\n",
       "      <td>0.064580</td>\n",
       "      <td>0.176601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>129.929993</td>\n",
       "      <td>76960600</td>\n",
       "      <td>36973</td>\n",
       "      <td>6.911228</td>\n",
       "      <td>0.199473</td>\n",
       "      <td>0.060634</td>\n",
       "      <td>0.168082</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Adj Close  stock_volume  twitter_volume      likes  Subjectivity  \\\n",
       "Dates                                                                           \n",
       "2022-12-01  148.309998      71250400            1451   3.358270      0.341031   \n",
       "2022-12-02  147.809998      65447400            1551   2.422508      0.336724   \n",
       "2022-12-05  146.630005      68826400            1738  16.589788      0.285005   \n",
       "2022-12-06  142.910004      64727200            2072   3.363636      0.308533   \n",
       "2022-12-07  140.940002      69721100            1912   3.910183      0.306545   \n",
       "2022-12-08  142.649994      62128300            1649   2.735952      0.323832   \n",
       "2022-12-09  142.160004      76097000            1415   2.080395      0.317647   \n",
       "2022-12-12  144.490005      70462700            1441   2.265884      0.307966   \n",
       "2022-12-13  145.470001      93886200            1860   2.004828      0.317893   \n",
       "2022-12-14  143.210007      82291200            1466   1.507839      0.293880   \n",
       "2022-12-15  136.500000      98931900            2025   2.876294      0.312479   \n",
       "2022-12-16  134.509995     160156900            1994   3.901804      0.293488   \n",
       "2022-12-19  132.369995      79592600            1962   4.602342      0.294270   \n",
       "2022-12-20  132.300003      77432800            2006   3.728721      0.315957   \n",
       "2022-12-21  135.449997      85928000            2013   2.823909      0.314458   \n",
       "2022-12-22  132.229996      77852100            2319   2.067527      0.308212   \n",
       "2022-12-23  131.860001      63814900            1822   5.494789      0.324043   \n",
       "2022-12-27  130.029999      69007800            2745   3.625591      0.316464   \n",
       "2022-12-28  126.040001      85438400            3508   5.331438      0.304389   \n",
       "2022-12-29  129.610001      75703700           37362   8.204908      0.206590   \n",
       "2022-12-30  129.929993      76960600           36973   6.911228      0.199473   \n",
       "\n",
       "            Polarity  Sentiment  open_trend  high_trend  low_trend  \\\n",
       "Dates                                                                \n",
       "2022-12-01  0.166630   0.418668           0           0          0   \n",
       "2022-12-02  0.179263   0.434727           0           0          0   \n",
       "2022-12-05  0.119601   0.320138           1           1          1   \n",
       "2022-12-06  0.138852   0.345839           0           0          0   \n",
       "2022-12-07  0.141816   0.385379           0           0          0   \n",
       "2022-12-08  0.177100   0.450755           1           1          1   \n",
       "2022-12-09  0.172363   0.416784           0           1          0   \n",
       "2022-12-12  0.150830   0.387431           1           0          1   \n",
       "2022-12-13  0.169065   0.425966           1           1          1   \n",
       "2022-12-14  0.155435   0.414451           0           0          0   \n",
       "2022-12-15  0.145653   0.379990           0           0          0   \n",
       "2022-12-16  0.147329   0.367735           0           0          0   \n",
       "2022-12-19  0.126957   0.335540           0           0          0   \n",
       "2022-12-20  0.149016   0.357392           0           0          0   \n",
       "2022-12-21  0.161636   0.380952           1           1          1   \n",
       "2022-12-22  0.153190   0.373333           1           0          0   \n",
       "2022-12-23  0.176336   0.428963           0           0          0   \n",
       "2022-12-27  0.137869   0.362050           1           0          0   \n",
       "2022-12-28  0.129806   0.328312           0           0          0   \n",
       "2022-12-29  0.064580   0.176601           0           0          1   \n",
       "2022-12-30  0.060634   0.168082           1           0          0   \n",
       "\n",
       "            close_trend  volume_trend  \n",
       "Dates                                  \n",
       "2022-12-01            0             0  \n",
       "2022-12-02            0             0  \n",
       "2022-12-05            0             1  \n",
       "2022-12-06            0             0  \n",
       "2022-12-07            0             1  \n",
       "2022-12-08            1             0  \n",
       "2022-12-09            0             1  \n",
       "2022-12-12            1             0  \n",
       "2022-12-13            1             1  \n",
       "2022-12-14            0             0  \n",
       "2022-12-15            0             1  \n",
       "2022-12-16            0             1  \n",
       "2022-12-19            0             0  \n",
       "2022-12-20            0             0  \n",
       "2022-12-21            1             1  \n",
       "2022-12-22            0             0  \n",
       "2022-12-23            0             0  \n",
       "2022-12-27            0             1  \n",
       "2022-12-28            0             1  \n",
       "2022-12-29            1             0  \n",
       "2022-12-30            1             1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rearranging the columns\n",
    "merged_dataframes = merged_dataframes[[\"Adj Close\", 'stock_volume', 'twitter_volume',\n",
    "                                      'likes', 'Subjectivity', 'Polarity', 'Sentiment',\n",
    "                                      'open_trend', 'high_trend', 'low_trend', 'close_trend',\n",
    "                                       'volume_trend']]\n",
    "merged_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cbbb6e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v3/zfqmnfxs5z3dkfs5cn70l8bh0000gn/T/ipykernel_23220/2814748260.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_dataframes['Sentiment_Score'] = sentiments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Positive    21\n",
       "Name: Sentiment_Score, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assigning \"Positive\", \"Neutral\", or \"Negative\" sentiment to each trading day\n",
    "#A stock at 0.0 sentiment does not neccesarily mean positive or negative. So we will give a 0.05 buffer\n",
    "POSITIVE_SENTIMENT_THRESHOLD = 0.05 \n",
    "NEGATIVE_SENTIMENT_THRESHOLD = -0.05\n",
    "\n",
    "sentiments = []\n",
    "for sentiment_score in merged_dataframes['Polarity']:\n",
    "    if sentiment_score >= POSITIVE_SENTIMENT_THRESHOLD:\n",
    "        sentiments.append('Positive')\n",
    "    elif sentiment_core <= NEGATIVE_SENTIMENT_THRESHOLD:\n",
    "        sentiments.append('Negative')\n",
    "    else:\n",
    "        sentiments.append('Neutral')\n",
    "        \n",
    "#Creating a new column by adding the newly created sentiment list\n",
    "merged_dataframes['Sentiment_Score'] = sentiments\n",
    "merged_dataframes['Sentiment_Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd0d01",
   "metadata": {},
   "source": [
    "## 3.2. Pre-processing: Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d432e69d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>user</th>\n",
       "      <th>likes</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:29:43</td>\n",
       "      <td>LlcBillionaire</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>10 New Yearâs food traditions around the world</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:29:32</td>\n",
       "      <td>skitontop1</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>Entries &amp;amp; exits Daily! \\nDiscord link belo...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:29:28</td>\n",
       "      <td>StockJobberOG</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>$AAPL $MSFT $SPY $TSLA $AMZN $BRK.B\\n\\n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:29:11</td>\n",
       "      <td>LlcBillionaire</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>The biggest â and maybe the best â financi...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:28:29</td>\n",
       "      <td>skitontop1</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>#1 Chatroom interms of \\n\\nalert,calls,Analysi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dates      Time            user  likes           source  \\\n",
       "0  2022-12-30  20:29:43  LlcBillionaire      0  Twitter Web App   \n",
       "1  2022-12-30  20:29:32      skitontop1      0  Twitter Web App   \n",
       "2  2022-12-30  20:29:28   StockJobberOG      0  Twitter Web App   \n",
       "3  2022-12-30  20:29:11  LlcBillionaire      0  Twitter Web App   \n",
       "4  2022-12-30  20:28:29      skitontop1      0  Twitter Web App   \n",
       "\n",
       "                                                text  Subjectivity  Polarity  \\\n",
       "0  10 New Yearâs food traditions around the world       0.454545  0.136364   \n",
       "1  Entries &amp; exits Daily! \\nDiscord link belo...      0.500000  0.300000   \n",
       "2            $AAPL $MSFT $SPY $TSLA $AMZN $BRK.B\\n\\n      0.000000  0.000000   \n",
       "3  The biggest â and maybe the best â financi...      0.150000  0.500000   \n",
       "4  #1 Chatroom interms of \\n\\nalert,calls,Analysi...      1.000000  0.600000   \n",
       "\n",
       "   Analysis  Sentiment  \n",
       "0  Positive        1.0  \n",
       "1  Positive        1.0  \n",
       "2   Neutral        0.0  \n",
       "3  Positive        1.0  \n",
       "4  Positive        1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing tweet data from previous notebook \"02_Exploratory_Data_Analysis\"\n",
    "trading_hours_tweets = pd.read_csv('02_tweets_data.csv', encoding='latin-1')\n",
    "trading_hours_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67e07bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 118345 entries, 0 to 118344\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Dates         118345 non-null  object \n",
      " 1   Time          118345 non-null  object \n",
      " 2   user          118345 non-null  object \n",
      " 3   likes         118345 non-null  int64  \n",
      " 4   source        118345 non-null  object \n",
      " 5   text          118026 non-null  object \n",
      " 6   Subjectivity  118345 non-null  float64\n",
      " 7   Polarity      118345 non-null  float64\n",
      " 8   Analysis      118345 non-null  object \n",
      " 9   Sentiment     118345 non-null  float64\n",
      "dtypes: float64(3), int64(1), object(6)\n",
      "memory usage: 9.0+ MB\n"
     ]
    }
   ],
   "source": [
    "trading_hours_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "639d46a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dates            object\n",
       "Time             object\n",
       "user             object\n",
       "likes             int64\n",
       "source           object\n",
       "text             object\n",
       "Subjectivity    float64\n",
       "Polarity        float64\n",
       "Analysis         object\n",
       "Sentiment       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_hours_tweets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fecd92bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_hours_tweets['text'] = trading_hours_tweets['text'].values.astype(str)\n",
    "trading_hours_tweets['text'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c599f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>user</th>\n",
       "      <th>likes</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:29:43</td>\n",
       "      <td>LlcBillionaire</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>10 New Yearâs food traditions around the world</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:29:32</td>\n",
       "      <td>skitontop1</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>Entries &amp;amp; exits Daily! \\nDiscord link belo...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:29:28</td>\n",
       "      <td>StockJobberOG</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>$AAPL $MSFT $SPY $TSLA $AMZN $BRK.B\\n\\n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:29:11</td>\n",
       "      <td>LlcBillionaire</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>The biggest â and maybe the best â financi...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>20:28:29</td>\n",
       "      <td>skitontop1</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>#1 Chatroom interms of \\n\\nalert,calls,Analysi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118340</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>13:30:02</td>\n",
       "      <td>whitestone_UK</td>\n",
       "      <td>0</td>\n",
       "      <td>Buffer</td>\n",
       "      <td>iPhone 14 Pro WhiteStone Dome Glass BEST Glass...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118341</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>13:30:02</td>\n",
       "      <td>NicheMktMedia</td>\n",
       "      <td>0</td>\n",
       "      <td>Buffer</td>\n",
       "      <td>What Are The Best Apple IPhone Cases</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118342</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>13:30:01</td>\n",
       "      <td>BooknewsGoods_j</td>\n",
       "      <td>0</td>\n",
       "      <td>bookbotgoods</td>\n",
       "      <td>ãåªè¡å»»æ¦ãæ°çã°ããº\\n\\nãiPhone...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118343</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>13:30:00</td>\n",
       "      <td>konyamoentame</td>\n",
       "      <td>2</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>ã¾ããªã23æããï¼\\n\\nã¨ãã¨ã ã²ã...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118344</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>13:30:00</td>\n",
       "      <td>PrestonMcMurry</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>@SelectWomanTaft @Baldassano I guess it wasn't...</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118345 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Dates      Time             user  likes                  source  \\\n",
       "0       2022-12-30  20:29:43   LlcBillionaire      0         Twitter Web App   \n",
       "1       2022-12-30  20:29:32       skitontop1      0         Twitter Web App   \n",
       "2       2022-12-30  20:29:28    StockJobberOG      0         Twitter Web App   \n",
       "3       2022-12-30  20:29:11   LlcBillionaire      0         Twitter Web App   \n",
       "4       2022-12-30  20:28:29       skitontop1      0         Twitter Web App   \n",
       "...            ...       ...              ...    ...                     ...   \n",
       "118340  2022-12-29  13:30:02    whitestone_UK      0                  Buffer   \n",
       "118341  2022-12-29  13:30:02    NicheMktMedia      0                  Buffer   \n",
       "118342  2022-12-29  13:30:01  BooknewsGoods_j      0            bookbotgoods   \n",
       "118343  2022-12-29  13:30:00    konyamoentame      2    Twitter for iPhone     \n",
       "118344  2022-12-29  13:30:00   PrestonMcMurry      1         Twitter Web App   \n",
       "\n",
       "                                                     text  Subjectivity  \\\n",
       "0       10 New Yearâs food traditions around the world       0.454545   \n",
       "1       Entries &amp; exits Daily! \\nDiscord link belo...      0.500000   \n",
       "2                 $AAPL $MSFT $SPY $TSLA $AMZN $BRK.B\\n\\n      0.000000   \n",
       "3       The biggest â and maybe the best â financi...      0.150000   \n",
       "4       #1 Chatroom interms of \\n\\nalert,calls,Analysi...      1.000000   \n",
       "...                                                   ...           ...   \n",
       "118340  iPhone 14 Pro WhiteStone Dome Glass BEST Glass...      0.300000   \n",
       "118341              What Are The Best Apple IPhone Cases       0.300000   \n",
       "118342  ãåªè¡å»»æ¦ãæ°çã°ããº\\n\\nãiPhone...      0.000000   \n",
       "118343  ã¾ããªã23æããï¼\\n\\nã¨ãã¨ã ã²ã...      0.000000   \n",
       "118344  @SelectWomanTaft @Baldassano I guess it wasn't...      0.535714   \n",
       "\n",
       "        Polarity  Analysis  Sentiment  \n",
       "0       0.136364  Positive        1.0  \n",
       "1       0.300000  Positive        1.0  \n",
       "2       0.000000   Neutral        0.0  \n",
       "3       0.500000  Positive        1.0  \n",
       "4       0.600000  Positive        1.0  \n",
       "...          ...       ...        ...  \n",
       "118340  1.000000  Positive        1.0  \n",
       "118341  1.000000  Positive        1.0  \n",
       "118342  0.000000   Neutral        0.0  \n",
       "118343  0.000000   Neutral        0.0  \n",
       "118344  0.285714  Positive        1.0  \n",
       "\n",
       "[118345 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_hours_tweets[trading_hours_tweets.isna().any(axis=1)]\n",
    "trading_hours_tweets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eadadd",
   "metadata": {},
   "source": [
    "### 3.2.1 Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625a8be",
   "metadata": {},
   "source": [
    "\n",
    "Text Mining is the process of deriving meaningful information from natural language text. Natural Language Processing (NLP) is a part of computer science and artificial intelligence which deals with human languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "594272f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of characters present in a tweet.\n",
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "#Count the number of words present in each line of tweet\n",
    "def count_words(text):\n",
    "    return len(str(text).split())\n",
    "\n",
    "#Count the number of punctuation\n",
    "import string\n",
    "def count_punctuations(text):\n",
    "    punctuations = string.punctuation\n",
    "    d=dict()\n",
    "    for i in punctuations:\n",
    "        d[str(i)+' count']=text.count(i)\n",
    "    return d \n",
    "\n",
    "#Count the number of words in quotation marks\n",
    "def count_words_in_quotes(text):\n",
    "    x = re.findall(('.'|\".\"), text)\n",
    "    count=0\n",
    "    if x is None:\n",
    "        return 0\n",
    "    else:\n",
    "        for i in x:\n",
    "            t=i[1:-1]\n",
    "            count+=count_words(t)\n",
    "        return count\n",
    "    \n",
    "#Count the number of sentences\n",
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "#Count the number of unique words\n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "#Count of hashtags\n",
    "def count_htags(text):\n",
    "    x = re.findall(r'(#w[A-Za-z0-9]*)', text)\n",
    "    return len(x)\n",
    "\n",
    "#Count of mentions\n",
    "def count_mentions(text):\n",
    "    x = re.findall(r'(@w[A-Za-z0-9]*)', text)\n",
    "    return len(x)\n",
    "\n",
    "#Count of stopwords\n",
    "def count_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad077a38",
   "metadata": {},
   "source": [
    "Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_hours_tweets['char_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_chars(x))\n",
    "trading_hours_tweets['word_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_words(x))\n",
    "trading_hours_tweets['sent_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_sent(x))\n",
    "trading_hours_tweets['stopword_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_stopwords(x))\n",
    "trading_hours_tweets['unique_word_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_unique_words(x))\n",
    "trading_hours_tweets['htag_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_htags(x))\n",
    "trading_hours_tweets['mention_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_mentions(x))\n",
    "trading_hours_tweets['punct_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of average word length\n",
    "trading_hours_tweets['avg_wordlength'] = trading_hours_tweets['char_count']/trading_hours_tweets['word_count']\n",
    "\n",
    "#Count of average sentence length\n",
    "trading_hours_tweets['avg_sentlength'] = trading_hours_tweets['word_count']/trading_hours_tweets['sent_count']\n",
    "\n",
    "#Ratio of unique words to total word count\n",
    "trading_hours_tweets['unique_vs_words'] = trading_hours_tweets['unique_word_count']/trading_hours_tweets['word_count']\n",
    "\n",
    "#Ratio of stopwords to total word count\n",
    "trading_hours_tweets['stopwords_vs_words'] = trading_hours_tweets['stopword_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_hours_tweets['avg_wordlength'] = trading_hours_tweets['char_count']/trading_hours_tweets['word_count']\n",
    "trading_hours_tweets['avg_sentlength'] = trading_hours_tweets['word_count']/trading_hours_tweets['sent_count']\n",
    "trading_hours_tweets['unique_vs_words'] = trading_hours_tweets['unique_word_count']/trading_hours_tweets['word_count']\n",
    "trading_hours_tweets['stopwords_vs_words'] = trading_hours_tweets['stopword_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607fb091",
   "metadata": {},
   "source": [
    "### 3.2.2 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce2e40",
   "metadata": {},
   "source": [
    "Here we turn our twitter strings to lists of individual tokens (words, punctuations). Tokenization is the first step in NLP. It is the process of breaking strings into tokens which in turn are small structures or units. Tokenization involves three steps which are breaking a complex sentence into words, understanding the importance of each word with respect to the sentence and finally produce structural description on an input sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500dcf01",
   "metadata": {},
   "source": [
    "Performing a simple pre-processing step, like removing links, removing user name, numbers, double space, punctuation, lower casing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c64861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'httpS+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RTs@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet\n",
    "my_punctuation = string.punctuation\n",
    "def preprocess(sent):\n",
    "    sent = remove_users(sent)\n",
    "    sent = remove_links(sent)\n",
    "    sent = sent.lower() # lower case\n",
    "    sent = re.sub('['+ my_punctuation + ']+',' ', sent) # strip punctuation\n",
    "    sent = re.sub('s+', ' ', sent) #remove double spacing\n",
    "    sent = re.sub('([0-9]+)', '', sent) # remove numbers\n",
    "    sent_token_list = [word for word in sent.split(' ')]\n",
    "    sent = ' '.join(sent_token_list)\n",
    "    return sent\n",
    "trading_hours_tweets['clean_text'] = trading_hours_tweets['text'].apply(lambda x: remove_links(x))\n",
    "trading_hours_tweets['clean_text'] = trading_hours_tweets['text'].apply(lambda x: remove_users(x))\n",
    "trading_hours_tweets['clean_text'] = trading_hours_tweets['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6555113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#NLTK tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "trading_hours_tweets['tokens'] = trading_hours_tweets['clean_text'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6cd65",
   "metadata": {},
   "source": [
    "### 3.2.3 Sentence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef87846",
   "metadata": {},
   "source": [
    "Using the tokens that we generated, we want to explore sentence length and vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = [len(tokens) for tokens in trading_hours_tweets['tokens']]\n",
    "vocab = sorted(list(set([word for tokens in trading_hours_tweets['tokens'] for word in tokens])))\n",
    "\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.xlabel('Sentence Length (in words)')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.title('Sentence Lengths')\n",
    "plt.hist(sentence_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26331bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have a vocabulary size of', len(vocab), 'unique words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88285e15",
   "metadata": {},
   "source": [
    "### 3.2.4 Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01973b97",
   "metadata": {},
   "source": [
    "“Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c07ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "trading_hours_tweets['tweet_without_stopwords'] = trading_hours_tweets['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e48ae0",
   "metadata": {},
   "source": [
    "### 3.2.5 Lemmitization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f1879",
   "metadata": {},
   "source": [
    "In simpler terms, lemmitization is the process of converting a word to its base form. Lemmatization considers the context and converts the word to its meaningful base form. For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care'. This is extremely valuable because we want to identify key words that lead to negative, positive and neutral sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c9e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "trading_hours_tweets['tweet_lemmatized'] = trading_hours_tweets['tweet_without_stopwords'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a0a9c",
   "metadata": {},
   "source": [
    "## 3.3 Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a948f1a",
   "metadata": {},
   "source": [
    "The purpose of combining dataframes is to view the correlation between each features. We will perform feature \"elimination\" to help reduce chances of overfitting and running into the curse of dimensionality. First, let's take the average of any quantitative features for tweets data and group by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tweets = trading_hours_tweets.groupby('Dates').mean()\n",
    "combined_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9cbaf6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Merging tweets with stock adj. closing price. By default, merging will perform an inner join\n",
    "stock_tweets_data = pd.merge(eda_stock_data['Adj Close'], combined_tweets, left_index=True, right_index=True)\n",
    "stock_tweets_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd7887",
   "metadata": {},
   "source": [
    "### 3.3.1 Correlation in Stock Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e613a",
   "metadata": {},
   "source": [
    "Understanding how the features in a dataset interact with each other is crucial when deciding which features to use in a model. There are many ways to construct a model that is effective and accurate. One of the fastest ways to strengthen a model is to identify and reduce the features in the dataset that are highly correlated. Correlated features will add noise and inaccuracy to a model, which in turn will make it harder to achieve the desired outcome.\n",
    "\n",
    "Highly correlated variables should be avoided when creating models is because they can skew the output. If there are two independent variables that are representing the same occurrence (i.e SqFt of a house vs bedrooms in a house) it can create “noise” or inaccuracy in the model. Models rely solely on outside information in order to create a useful output and having colinear (correlated) variables can create an inflated variance in at least one of the regression outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa450e60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Correlation heatmap for stock data\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Create a covariance matrix\n",
    "corr = eda_stock_data.corr()\n",
    "\n",
    "# Creating a mask the size of our covariance matrix\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220,10,as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr,annot=True,mask=mask,cmap=cmap,vmax=1,center=0,square=True, \n",
    "            linewidth=.5, cbar_kws={'shrink': .5})\n",
    "ax.set_title('Multi-Collinearity of Stock Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac96d7d",
   "metadata": {},
   "source": [
    "No block has siginifacant high correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa99351",
   "metadata": {},
   "source": [
    "### 3.3.3 Correlation on Tweets Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ef992",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Correlation heatmap for tweets data + adj. closing price\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Create a covariance matrix\n",
    "corr = stock_tweets_data.corr()\n",
    "\n",
    "# Creating a mask the size of our covariance matrix\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11,9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220,10,as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr,annot=True,mask=mask,cmap=cmap,vmax=1,center=0,square=True, \n",
    "            linewidth=.5, cbar_kws={'shrink': .5})\n",
    "ax.set_title('Multi-Collinearity of Stock Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbad865",
   "metadata": {},
   "source": [
    "As the maps above shows, as the color becomes darker in either direction (red or blue) that means that those variables are more highly correlated and should not be paired together in the same model. With that said, since our y is Adj. Close, we need to eliminate any features that are highly correlated to Adj. Close. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b30382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dropping columns with correlation higher and lower than 0.50 and -0.50\n",
    "trading_hours_tweets = trading_hours_tweets.drop(['likes', 'stopword_count', 'stopwords_vs_words'], axis=1)\n",
    "trading_hours_tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab7667",
   "metadata": {},
   "source": [
    "## 3.4 Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acffcd2e",
   "metadata": {},
   "source": [
    "One of the features that we will be using to predict the adjusted closing price is the vectorized representation of the tweet. To get the numbers, we need to build a text classification model using word embedding.\n",
    "\n",
    "**Text classification** can be applied to many different problems, including gauging the public opinion on social media. Historically, it is studied as news articles classification into a pre-determined set of classes, and based on the summary or the title or even the whole body of the article the machine learning models determine if the news article is about economy, sports, real state, and so on. It can also be used for classifying a company's documents into categories that will be analyzed only by the corresponding department. Finally, it can direct, for example, quantify public opinions by classifying the query of a ticker, allowing to gauge public sentiments and lead to accurate price prediction.\n",
    "\n",
    "**Word embedding** is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. Loosely speaking, they are vector representations of a particular word. In this project, we will use Word2Vec embedding method to map a word to a fixed-length vector. In simple terms, words that have similar meanings or are related closely, when mapped into a vector space would appear closer, like in a cluster. This can help us understand the semantics of the words in a sentence better than any previously mentioned technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ff323",
   "metadata": {},
   "source": [
    "Text classfication models performed in this project are Bags of words, TF-IDF, Word2Vec and BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1fc214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking gensim version\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b973d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring pre-trained sets\n",
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89292e",
   "metadata": {},
   "source": [
    "### 3.4.1 Testing Pre-Trained Vector Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb50f577",
   "metadata": {},
   "source": [
    "***Let's first look at the glove-twitter-200 pre-trained set***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165307ab",
   "metadata": {},
   "source": [
    "#loading one of the pre-trained vectors \\\n",
    "wv = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b133ecb",
   "metadata": {},
   "source": [
    "#Loading pre-trained vector can take minutes, so we can also save it using the function below \\\n",
    "wv.save('/Users/user/Documents/Springboard_Data_Science/Capstone_2_Twitter_Sentiment_Analysis/Data/glove-twitter-200.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#and load it whenever we need it \n",
    "from gensim.models import KeyedVectors \n",
    "wv = KeyedVectors.load('/Users/user/Documents/Springboard_Data_Science/Capstone_2_Twitter_Sentiment_Analysis/Data/glove-twitter-200.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9581658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv['apple'])\n",
    "\n",
    "#200 meaning there are 200 dimensional\n",
    "print(len(wv['apple']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d064fa",
   "metadata": {},
   "source": [
    "***Let's now look at the word2vec-google-news-300 pre-trained set***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2c584",
   "metadata": {},
   "source": [
    "#downloading from database \\\n",
    "wv2 = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72c9e8",
   "metadata": {},
   "source": [
    "#saving \\\n",
    "wv2.save('/Users/user/Documents/Springboard_Data_Science/Capstone_2_Twitter_Sentiment_Analysis/Data/word2vec-google-news-300.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading to notebook\n",
    "wv2 = KeyedVectors.load('/Users/user/Documents/Springboard_Data_Science/Capstone_2_Twitter_Sentiment_Analysis/Data/word2vec-google-news-300.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try and see the similarity between apple and tesla using both loaded sets\n",
    "print(\"Using glove-twitter-200 pre-trained set, the similarity between apple and tesla are\", wv.similarity('apple', 'tesla'))\n",
    "print(\"Using word2vec-google-300 pre-trained set, the similarity between apple and tesla are\", wv2.similarity('apple', 'tesla'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204336e",
   "metadata": {},
   "source": [
    "Looks like the Word2Vec set does not really understand \"apple\" here stands for the corporate name Apple Inc. It is totally understandable since one of the weaknesses of Word2Vec is that it does not take into consideration the context of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#But if we compare apple and mango as fruit names\n",
    "print(\"Using glove-twitter-200 pre-trained set, the similarity between apple and mango are\", wv.similarity('apple', 'mango'))\n",
    "print(\"Using word2vec-google-300 pre-trained set, the similarity between apple and mango are\", wv2.similarity('apple', 'mango'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64263a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#But if we compare two different entities\n",
    "print(\"Using glove-twitter-200 pre-trained set, the similarity between apple and car are\", wv.similarity('apple', 'car'))\n",
    "print(\"Using word2vec-google-300 pre-trained set, the similarity between apple and car are\", wv2.similarity('apple', 'car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),\n",
    "    ('car', 'bicycle'),\n",
    "    ('car', 'airplane'),\n",
    "    ('car', 'cereal'),\n",
    "    ('car', 'communism')\n",
    "]\n",
    "\n",
    "print(\"Using twitter-200\")\n",
    "for w1, w2 in pairs:\n",
    "    print(\"%r\\t%r\\t%.2f\" % (w1, w2, wv.similarity(w1, w2)))\n",
    "    \n",
    "print(\"Using google-300\")\n",
    "for w1, w2 in pairs:\n",
    "    print(\"%r\\t%r\\t%.2f\" % (w1, w2, wv2.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d34b4",
   "metadata": {},
   "source": [
    "From testing a few pairs of text, we can see that the glove-200 pretrained set has higher accuracy for stock-related keyword, while the google-300 pre-trained set has higher accuracy for common pairs. Thus, we will use the glove-200 pre-trained set for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to vectorize the tokens\n",
    "def sent_vec(sent):\n",
    "    vector_size = wv.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in wv:\n",
    "            ctr +=1\n",
    "            wv_res += wv[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb23ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_hours_tweets['vec'] = trading_hours_tweets['tweet_lemmatized'].apply(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902ee9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trading_hours_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec3000",
   "metadata": {},
   "source": [
    "### 3.4.2 Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e0ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trading_hours_tweets['vec'].to_list()\n",
    "y = trading_hours_tweets['Sentiment'].to_list()\n",
    "\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfba322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing train test split on data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb882d",
   "metadata": {},
   "source": [
    "## 3.5 Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f7407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_hours_tweets.to_csv('03_tweets_data.csv', index=False)\n",
    "eda_stock_data.to_csv('03_stock_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
