{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f4429d",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - 04 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2117bd73",
   "metadata": {},
   "source": [
    "The stock market is a focus for investors to maximize their potential profits and consequently, the interest shown from the technical and financial sides in stock market prediction is always on the rise. However, stock market prediction is a problem known for its challenging nature due to its dependency on diverse factors that affect the market, these factors are unpredictable and cannot be taken into consideration such as political variables, and social media effects such as twitter on the stock market.\n",
    "\n",
    "In this final part of this project, we will combine the stock data and its features, with vectorized representation of the tweets for the month of December 2022 to predict whether or not the adjusted closing price at the end of a trading-day is greater than or less than the previous trading-day. Models run are Logistic Regression, KNN, SVM, Random Forest, and K-means Clustering.\n",
    "\n",
    "Two types of predictions: \n",
    "1. Using overall daily tweet sentiment scores to predict adjusted closing price\n",
    "2. Using vectorized representation of tweets to predict adjusted closing price\n",
    "\n",
    "**Link(s) to previous notebook(s)**: \\\n",
    "00_Historical_Data_2014: https://github.com/parisvu07/Springboard_Data_Science/tree/main/Capstone_2_Twitter_Sentiment_Analysis \\\n",
    "01_Data_Wrangling:\n",
    "https://github.com/parisvu07/Springboard/blob/main/Capstone_2_Twitter_Sentiment_Analysis/01_Data_Wrangling.ipynb \\\n",
    "02_Exploratory_Data_Analysis: https://github.com/parisvu07/Springboard/blob/main/Capstone_2_Twitter_Sentiment_Analysis/02_Exploratory_Data_Analysis.ipynb \\\n",
    "03_Preprocessing_and_Training_Data: https://github.com/parisvu07/Springboard/blob/main/Capstone_2_Twitter_Sentiment_Analysis/03_Preprocessing_and_Training_Data.ipynb\n",
    "\n",
    "Quick fix for \"Unable to render rich display\": copy and paste the notebook link to https://nbviewer.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b6f3a3",
   "metadata": {},
   "source": [
    "## 4.1 Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11511d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "\n",
    "#ignore warning messages to ensure clean outputs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "LabeledSentence = gensim.models.doc2vec.TaggedDocument\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import spacy\n",
    "np.random.seed(42)\n",
    "\n",
    "from gensim.models import KeyedVectors \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da83c4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>stock_volume</th>\n",
       "      <th>%_change_Open</th>\n",
       "      <th>%_change_High</th>\n",
       "      <th>%_change_Low</th>\n",
       "      <th>%_change_Close</th>\n",
       "      <th>%_change_Volume</th>\n",
       "      <th>twitter_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>148.309998</td>\n",
       "      <td>71250400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02</th>\n",
       "      <td>147.809998</td>\n",
       "      <td>65447400</td>\n",
       "      <td>-1.518116</td>\n",
       "      <td>-0.757731</td>\n",
       "      <td>-0.654803</td>\n",
       "      <td>-0.337132</td>\n",
       "      <td>-8.144516</td>\n",
       "      <td>1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-05</th>\n",
       "      <td>146.630005</td>\n",
       "      <td>68826400</td>\n",
       "      <td>1.240064</td>\n",
       "      <td>1.972972</td>\n",
       "      <td>0.082396</td>\n",
       "      <td>-0.798317</td>\n",
       "      <td>5.162925</td>\n",
       "      <td>1738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-06</th>\n",
       "      <td>142.910004</td>\n",
       "      <td>64727200</td>\n",
       "      <td>-0.473707</td>\n",
       "      <td>-2.398619</td>\n",
       "      <td>-2.641151</td>\n",
       "      <td>-2.536999</td>\n",
       "      <td>-5.955854</td>\n",
       "      <td>2072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-07</th>\n",
       "      <td>140.940002</td>\n",
       "      <td>69721100</td>\n",
       "      <td>-3.318151</td>\n",
       "      <td>-2.668030</td>\n",
       "      <td>-1.352874</td>\n",
       "      <td>-1.378491</td>\n",
       "      <td>7.715304</td>\n",
       "      <td>1912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Adj Close  stock_volume  %_change_Open  %_change_High  \\\n",
       "Dates                                                                \n",
       "2022-12-01  148.309998      71250400            NaN            NaN   \n",
       "2022-12-02  147.809998      65447400      -1.518116      -0.757731   \n",
       "2022-12-05  146.630005      68826400       1.240064       1.972972   \n",
       "2022-12-06  142.910004      64727200      -0.473707      -2.398619   \n",
       "2022-12-07  140.940002      69721100      -3.318151      -2.668030   \n",
       "\n",
       "            %_change_Low  %_change_Close  %_change_Volume  twitter_volume  \n",
       "Dates                                                                      \n",
       "2022-12-01           NaN             NaN              NaN            1451  \n",
       "2022-12-02     -0.654803       -0.337132        -8.144516            1551  \n",
       "2022-12-05      0.082396       -0.798317         5.162925            1738  \n",
       "2022-12-06     -2.641151       -2.536999        -5.955854            2072  \n",
       "2022-12-07     -1.352874       -1.378491         7.715304            1912  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing stock data from notebook \"02_Exploratory_Data_Analysis\"\n",
    "stock_data = pd.read_csv('03_stock_data.csv', encoding='latin-1')\n",
    "stock_data = stock_data.set_index('Dates')\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf864f4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing tweet data from previous notebook \"03_Preprocessing_and_Training_Data\"\n",
    "tweets_data = pd.read_csv('03_tweets_data.csv')\n",
    "tweets_data = tweets_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df50e726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>stock_volume</th>\n",
       "      <th>twitter_volume</th>\n",
       "      <th>likes</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>open_trend</th>\n",
       "      <th>high_trend</th>\n",
       "      <th>low_trend</th>\n",
       "      <th>close_trend</th>\n",
       "      <th>volume_trend</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>148.309998</td>\n",
       "      <td>71250400</td>\n",
       "      <td>1451</td>\n",
       "      <td>3.358270</td>\n",
       "      <td>0.341031</td>\n",
       "      <td>0.166630</td>\n",
       "      <td>0.418668</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02</th>\n",
       "      <td>147.809998</td>\n",
       "      <td>65447400</td>\n",
       "      <td>1551</td>\n",
       "      <td>2.422508</td>\n",
       "      <td>0.336724</td>\n",
       "      <td>0.179263</td>\n",
       "      <td>0.434727</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-05</th>\n",
       "      <td>146.630005</td>\n",
       "      <td>68826400</td>\n",
       "      <td>1738</td>\n",
       "      <td>16.589788</td>\n",
       "      <td>0.285005</td>\n",
       "      <td>0.119601</td>\n",
       "      <td>0.320138</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-06</th>\n",
       "      <td>142.910004</td>\n",
       "      <td>64727200</td>\n",
       "      <td>2072</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>0.308533</td>\n",
       "      <td>0.138852</td>\n",
       "      <td>0.345839</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-07</th>\n",
       "      <td>140.940002</td>\n",
       "      <td>69721100</td>\n",
       "      <td>1912</td>\n",
       "      <td>3.910183</td>\n",
       "      <td>0.306545</td>\n",
       "      <td>0.141816</td>\n",
       "      <td>0.385379</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Adj Close  stock_volume  twitter_volume      likes  Subjectivity  \\\n",
       "Dates                                                                           \n",
       "2022-12-01  148.309998      71250400            1451   3.358270      0.341031   \n",
       "2022-12-02  147.809998      65447400            1551   2.422508      0.336724   \n",
       "2022-12-05  146.630005      68826400            1738  16.589788      0.285005   \n",
       "2022-12-06  142.910004      64727200            2072   3.363636      0.308533   \n",
       "2022-12-07  140.940002      69721100            1912   3.910183      0.306545   \n",
       "\n",
       "            Polarity  Sentiment  open_trend  high_trend  low_trend  \\\n",
       "Dates                                                                \n",
       "2022-12-01  0.166630   0.418668           0           0          0   \n",
       "2022-12-02  0.179263   0.434727           0           0          0   \n",
       "2022-12-05  0.119601   0.320138           1           1          1   \n",
       "2022-12-06  0.138852   0.345839           0           0          0   \n",
       "2022-12-07  0.141816   0.385379           0           0          0   \n",
       "\n",
       "            close_trend  volume_trend Sentiment_Score  \n",
       "Dates                                                  \n",
       "2022-12-01            0             0        Positive  \n",
       "2022-12-02            0             0        Positive  \n",
       "2022-12-05            0             1        Negative  \n",
       "2022-12-06            0             0        Negative  \n",
       "2022-12-07            0             1        Negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing merged dataframes from previous notebook \"03_Preprocessing_and_Training_Data\"\n",
    "merged_dataframes = pd.read_csv('03_merged_dataframes.csv', lineterminator='\\n')\n",
    "merged_dataframes = merged_dataframes.set_index('Dates')\n",
    "merged_dataframes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737e3a1",
   "metadata": {},
   "source": [
    "## 4.2 Method 1: Using overall daily tweet sentiment score to predict adjusted closing price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da0ab1",
   "metadata": {},
   "source": [
    "This is a classification problem, in unsupervised learning. Here we have used the following classification models:\n",
    "\n",
    "Logistic Regression \\\n",
    "K-Nearest Neighbor (KNN) \\\n",
    "Support vector machine (SVM) \\\n",
    "Random Forest \\\n",
    "K-means Clustering \n",
    "\n",
    "Evaluating the performance of a model by training and testing on the same dataset can lead to the overfitting. Hence the model evaluation is based on splitting the dataset into train and validation set. But the performance of the prediction result depends upon the random choice of the pair of (train,validation) set. Inorder to overcome that, the Cross-Validation procedure is used where under the k-fold CV approach, the training set is split into k smaller sets, where a model is trained using k-1 of the folds as training data and the model is validated on the remaining part.\n",
    "\n",
    "Classification/ Confusion Matrix: This matrix summarizes the correct and incorrect classifications that a classifier produced for a certain dataset. Rows and columns of the classification matrix correspond to the true and predicted classes respectively. The two diagonal cells (upper left, lower right) give the number of correct classifications, where the predicted class coincides with the actual class of the observation. The off diagonal cells gives the count of the misclassification. The classification matrix gives estimates of the true classification and misclassification rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32fa45",
   "metadata": {},
   "source": [
    "#Transforming \"Sentiment_Score\" in the merged_dataframes dataset into binary codes\n",
    "merged_dataframes = pd.get_dummies(merged_dataframes, columns = ['Sentiment_Score'])\n",
    "merged_dataframes = merged_dataframes.drop(['Polarity'], axis=1)\n",
    "merged_dataframes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b82c321",
   "metadata": {},
   "source": [
    "### 4.2.1 Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ac775",
   "metadata": {},
   "source": [
    "1 means adj. closing price rose compare to its yesterday closing price. 0 means it fell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90062ab6",
   "metadata": {},
   "source": [
    "#Defining our X and y \n",
    "X = merged_dataframes.drop('close_trend', axis=1)\n",
    "y = merged_dataframes['close_trend'].values.reshape(-1,1)\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe40f3",
   "metadata": {},
   "source": [
    "#Splitting data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbccf5d",
   "metadata": {},
   "source": [
    "### 4.2.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce2767",
   "metadata": {},
   "source": [
    "#Making pipeline\n",
    "C_param_range = [0.001,0.01,0.1,1,10,100]\n",
    "\n",
    "table = pd.DataFrame(columns = ['C_parameter','Accuracy'])\n",
    "table['C_parameter'] = C_param_range\n",
    "\n",
    "j = 0\n",
    "for i in C_param_range:\n",
    "    \n",
    "    # Apply logistic regression model to training data\n",
    "    pipe = make_pipeline( \n",
    "    StandardScaler(),\n",
    "    LogisticRegression(penalty = 'l2', C = i,random_state = 40)\n",
    ")\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict using model\n",
    "    y_pred_lr = pipe.predict(X_test)\n",
    "    \n",
    "    # Saving accuracy score in table\n",
    "    table.iloc[j,1] = accuracy_score(y_test,y_pred_lr)\n",
    "    j += 1\n",
    "    \n",
    "table   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e79fb7",
   "metadata": {},
   "source": [
    "cnf_matrix = confusion_matrix(y_test,y_pred_lr)\n",
    "print(cnf_matrix)\n",
    "accuracy_lr = pipe.score(X_test,y_test)\n",
    "\n",
    "print(accuracy_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f76a4",
   "metadata": {},
   "source": [
    "cv_scores_test= cross_val_score(pipe,X_test,y_test,cv=2,scoring='roc_auc')\n",
    "cv_scores_train= cross_val_score(pipe,X_train,y_train,cv=2,scoring='roc_auc')\n",
    "print(cv_scores_test)\n",
    "cv_scores_lr_test= cv_scores_test.mean()\n",
    "cv_scores_lr_train= cv_scores_train.mean()\n",
    "cv_scores_std_test_lr= cv_scores_test.std()\n",
    "print ('Mean cross validation test score: ' +str(cv_scores_lr_test))\n",
    "print ('Mean cross validation train score: ' +str(cv_scores_lr_train))\n",
    "print ('Standard deviation in cv test scores: ' +str(cv_scores_std_test_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210476f",
   "metadata": {},
   "source": [
    "We can see that the merged_dataframes (tweets combined with stock sentiments) do not yield meaningful results. We could have create more binary codes for other features such as stock_volume, twitter_volukesm. But because our focus is on Natural Language Processing, we will stop here and move on to a text classfication model using Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab7667",
   "metadata": {},
   "source": [
    "## 4.3 Method 2: Using vectorized representation of tweets to predict adjusted closing price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de78308",
   "metadata": {},
   "source": [
    "Word2Vec is a collection of algorithms which can produce word embeddings. Word embeddings are vectors which describe the semantic meaning of words as points in space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76243f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pre-trained embedding\n",
    "wv = KeyedVectors.load('glove-twitter-200.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41712e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "      <th>vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>new yearâ  food tradition  around the world</td>\n",
       "      <td>['new', 'yearâ\\x80\\x99', 'food', 'tradition', ...</td>\n",
       "      <td>[0.018792242098632932, -0.049044443161359856, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>entrie   amp  exit  daily  \\ndi cord link belo...</td>\n",
       "      <td>['entrie', 'amp', 'exit', 'daily', 'di', 'cord...</td>\n",
       "      <td>[0.021105272420890518, -0.0418300421547089, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aapl  m ft   py  t la  amzn  brk b\\n\\n</td>\n",
       "      <td>['aapl', 'ft', 'py', 'la', 'amzn', 'brk', 'b']</td>\n",
       "      <td>[-0.06759836408309639, -0.04402014476860442, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the bigge t â and maybe the be t â financi...</td>\n",
       "      <td>['bigge', 'â\\x80\\x94', 'maybe', 'â\\x80\\x94', '...</td>\n",
       "      <td>[0.044564239627447065, -0.03471527336776711, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>chatroom interm  of \\n\\nalert call  analy i ...</td>\n",
       "      <td>['chatroom', 'interm', 'alert', 'call', 'analy']</td>\n",
       "      <td>[0.02505414104089141, -0.14497853252622817, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dates  Sentiment                                         clean_text  \\\n",
       "0  2022-12-30        1.0     new yearâ  food tradition  around the world    \n",
       "1  2022-12-30        1.0  entrie   amp  exit  daily  \\ndi cord link belo...   \n",
       "2  2022-12-30        0.0             aapl  m ft   py  t la  amzn  brk b\\n\\n   \n",
       "3  2022-12-30        1.0  the bigge t â and maybe the be t â financi...   \n",
       "4  2022-12-30        1.0    chatroom interm  of \\n\\nalert call  analy i ...   \n",
       "\n",
       "                                    tweet_lemmatized  \\\n",
       "0  ['new', 'yearâ\\x80\\x99', 'food', 'tradition', ...   \n",
       "1  ['entrie', 'amp', 'exit', 'daily', 'di', 'cord...   \n",
       "2     ['aapl', 'ft', 'py', 'la', 'amzn', 'brk', 'b']   \n",
       "3  ['bigge', 'â\\x80\\x94', 'maybe', 'â\\x80\\x94', '...   \n",
       "4   ['chatroom', 'interm', 'alert', 'call', 'analy']   \n",
       "\n",
       "                                                 vec  \n",
       "0  [0.018792242098632932, -0.049044443161359856, ...  \n",
       "1  [0.021105272420890518, -0.0418300421547089, -0...  \n",
       "2  [-0.06759836408309639, -0.04402014476860442, -...  \n",
       "3  [0.044564239627447065, -0.03471527336776711, -...  \n",
       "4  [0.02505414104089141, -0.14497853252622817, -0...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Because we got rid of the array before saving in notebook 03, we need to vectorize the text again\n",
    "def sent_vec(sent):\n",
    "    vector_size = wv.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in wv:\n",
    "            ctr +=1\n",
    "            wv_res += wv[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    return wv_res\n",
    "\n",
    "tweets_data['vec'] = tweets_data['tweet_lemmatized'].apply(sent_vec)\n",
    "tweets_data = tweets_data[['Dates', 'Sentiment', 'clean_text', 'tweet_lemmatized', 'vec']]\n",
    "\n",
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840319ad",
   "metadata": {},
   "source": [
    "word2vec can’t create a vector from a word that’s not in its vocabulary. Because of this, we need to specify “if word in model.vocab” when creating the full list of word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f9f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117841,)\n"
     ]
    }
   ],
   "source": [
    "# Grab all the tweets\n",
    "tweets = tweets_data['clean_text']\n",
    "print(tweets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b006a3d",
   "metadata": {},
   "source": [
    "#Create a list of strings, where each string is a tweet\n",
    "tweets_list = [tweet for tweet in tweets]\n",
    "\n",
    "#Collapse the list of strings into a single long string for processing\n",
    "big_tweet_string = ' '.join(tweets_list)\n",
    "\n",
    "#Tokenize the string into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(big_tweet_string)\n",
    "\n",
    "#Remove non-alphabetic tokens, such as punctuation\n",
    "words = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "#Filter out stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words = [word for word in words if not word in stop_words]\n",
    "\n",
    "#Print first 10 words\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d190c6",
   "metadata": {},
   "source": [
    "#Filter the list of vectors to include only those that Word2Vec has a vector for\n",
    "vector_list = [wv[word] for word in words if word in wv.key_to_index]\n",
    "\n",
    "#Create a list of the words corresponding to these vectors\n",
    "words_filtered = [word for word in words if word in wv.key_to_index]\n",
    "\n",
    "#Zip the words together with their vector representations\n",
    "word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "#Cast to a dict so we can turn it into a DataFrame\n",
    "word_vec_dict = dict(word_vec_zip)\n",
    "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40772d1d",
   "metadata": {},
   "source": [
    "### 4.3.1 Dimensionality Reduction with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4bfb8c",
   "metadata": {},
   "source": [
    "#Initialize t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
    "\n",
    "#Use only 400 rows to shorten processing time\n",
    "tsne_df = tsne.fit_transform(df[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6eb70",
   "metadata": {},
   "source": [
    "sns.set()\n",
    "#Initialize figure\n",
    "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
    "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
    "\n",
    "#Import adjustText, initialize list of texts\n",
    "from adjustText import adjust_text\n",
    "\n",
    "texts = []\n",
    "words_to_plot = list(np.arange(0, 400, 10))\n",
    "\n",
    "#Append words to list\n",
    "for word in words_to_plot:\n",
    "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
    "    \n",
    "#Plot text using adjust_text (because overlapping text is hard to read)\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3b7c3",
   "metadata": {},
   "source": [
    "We see certain items clustering together. For example, we have movements in the upper left, we have corporate finance terms near the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb575f",
   "metadata": {},
   "source": [
    "### 4.3.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "970a36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_data['vec'].to_list()\n",
    "y = tweets_data['Sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ddcf4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56f1eeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4690194",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v3/zfqmnfxs5z3dkfs5cn70l8bh0000gn/T/ipykernel_9722/87066916.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnf_matrix\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mAccuracy_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAccuracy_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred_lr' is not defined"
     ]
    }
   ],
   "source": [
    "cnf_matrix= confusion_matrix(y_test,y_pred_lr)\n",
    "print(cnf_matrix)\n",
    "Accuracy_lr=Logreg.score(X_test,y_test)\n",
    "\n",
    "print(Accuracy_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3de1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y_test = classifier.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", metrics.accuracy_score(y_test, predicted_y_test))\n",
    "print(\"Logistic Regression Precision:\", metrics.precision_score(y_test, predicted_y_test, average='micro'))\n",
    "print(\"Logistic Regression Recall:\", metrics.recall_score(y_test, predicted_y_test, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1450b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
